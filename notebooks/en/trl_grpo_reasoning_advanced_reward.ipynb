{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced GRPO Fine-tuning for Mathematical Reasoning with Multi-Reward Training\n",
    "\n",
    "_Authored by: [Behrooz Azarkhalili](https://github.com/behroozazarkhalili)_\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/behroozazarkhalili/GRPO-Qwen-Finetuning-Unsloth/blob/master/TRL_GRPO_Reasoning.ipynb)\n",
    "\n",
    "## üîó Related Cookbook Examples\n",
    "\n",
    "This notebook builds upon the existing [Post training an LLM for reasoning with GRPO in TRL](https://huggingface.co/learn/cookbook/fine_tuning_llm_grpo_trl) example. While the basic GRPO cookbook demonstrates the fundamentals of GRPO training, this advanced tutorial focuses on:\n",
    "\n",
    "### Key Differences from the Basic GRPO Example:\n",
    "\n",
    "| **Basic GRPO Example** | **This Advanced Example** |\n",
    "|-------------------------|---------------------------|\n",
    "| Single reward function | **Multiple reward functions** (4 different rewards) |\n",
    "| Basic format checking | **Advanced format validation** with regex patterns |\n",
    "| Simple training setup | **Interactive training dashboard** with real-time metrics |\n",
    "| Standard evaluation | **Comprehensive testing** with detailed error analysis |\n",
    "| Basic dataset processing | **Advanced dataset preparation** with custom tokenization |\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "This comprehensive tutorial demonstrates **advanced GRPO (Group Relative Policy Optimization)** techniques for mathematical reasoning, specifically:\n",
    "\n",
    "1. **Multi-Reward System Design**: Learn to create and balance multiple reward functions for complex tasks\n",
    "2. **Advanced Format Validation**: Implement sophisticated regex-based reward functions for structured outputs\n",
    "3. **Interactive Training Monitoring**: Build real-time training dashboards with metrics visualization\n",
    "4. **Production-Ready Implementation**: Design robust training pipelines with comprehensive error handling\n",
    "5. **Performance Optimization**: Use memory-efficient techniques like 4-bit quantization and LoRA adapters\n",
    "\n",
    "## üß† Deep Dive: What is GRPO?\n",
    "\n",
    "**Group Relative Policy Optimization (GRPO)** is a sophisticated reinforcement learning technique that improves upon traditional policy optimization methods by:\n",
    "\n",
    "### Core Principles:\n",
    "- **Relative Comparison**: Unlike absolute reward systems, GRPO compares responses within groups, making training more stable and robust\n",
    "- **Group-Based Learning**: Multiple candidate responses are generated and ranked relatively, reducing variance in training\n",
    "- **Multi-Objective Optimization**: Can handle multiple reward signals simultaneously, perfect for complex tasks like mathematical reasoning\n",
    "\n",
    "### Why GRPO for Mathematical Reasoning?\n",
    "Mathematical reasoning requires:\n",
    "1. **Format Adherence**: Solutions must follow specific structural patterns\n",
    "2. **Logical Consistency**: Step-by-step reasoning must be coherent\n",
    "3. **Accuracy**: Final answers must be mathematically correct\n",
    "4. **Clarity**: Explanations should be understandable\n",
    "\n",
    "GRPO excels at this because it can optimize for all these criteria simultaneously through multiple reward functions.\n",
    "\n",
    "## üéØ Advanced Features of This Implementation\n",
    "\n",
    "### üî• Multi-Reward Training System\n",
    "This notebook implements **4 specialized reward functions**:\n",
    "\n",
    "1. **`match_format_exactly`**: Ensures perfect adherence to the required output format\n",
    "2. **`match_format_approximately`**: Provides partial rewards for near-correct formatting\n",
    "3. **`check_answer_correctness`**: Validates mathematical accuracy with fuzzy matching\n",
    "4. **`check_numbers_extraction`**: Verifies numerical answer extraction and parsing\n",
    "\n",
    "### üìä Interactive Training Dashboard\n",
    "Features a **HuggingFace-style training table** that displays:\n",
    "- Real-time loss and reward metrics\n",
    "- Running averages and standard deviations\n",
    "- Best reward tracking across training steps\n",
    "- Gradient norms and KL divergence monitoring\n",
    "- JSON logging for complete training history\n",
    "\n",
    "### üß† Structured Reasoning Format\n",
    "The model learns to generate responses in this specific format:\n",
    "```\n",
    "<start_working_out>\n",
    "[Step-by-step mathematical reasoning]\n",
    "<end_working_out>\n",
    "\n",
    "<SOLUTION>\n",
    "[Final numerical answer]\n",
    "</SOLUTION>\n",
    "```\n",
    "\n",
    "This structure ensures that the model not only provides correct answers but also shows its reasoning process, making it interpretable and trustworthy.\n",
    "\n",
    "## üìö Dataset: GSM8K Mathematical Reasoning\n",
    "\n",
    "We use the **GSM8K** (Grade School Math 8K) dataset - a carefully curated collection of:\n",
    "- **8,000+ mathematical word problems** requiring multi-step reasoning\n",
    "- **Linguistically diverse problems** covering various mathematical concepts\n",
    "- **Step-by-step solutions** that require logical reasoning chains\n",
    "- **Numerical answers** that can be objectively evaluated\n",
    "\n",
    "The dataset is perfect for GRPO training because it provides clear success criteria (correct numerical answers) while requiring complex reasoning processes that benefit from multi-reward optimization.\n",
    "\n",
    "## üõ†Ô∏è Technical Implementation Details\n",
    "\n",
    "### Memory Efficiency\n",
    "- **4-bit Quantization**: Using NF4 quantization for significant memory savings\n",
    "- **LoRA Adapters**: Parameter-efficient fine-tuning targeting specific attention layers\n",
    "- **Gradient Checkpointing**: Reduced memory usage during backpropagation\n",
    "- **Optimized Batch Sizes**: Configured for consumer GPUs (16-24GB VRAM)\n",
    "\n",
    "### Training Stability\n",
    "- **Advanced Logging**: Comprehensive metrics tracking with automated visualization\n",
    "- **Error Handling**: Robust error handling for malformed outputs and edge cases\n",
    "- **Gradient Clipping**: Preventing gradient explosion in RL training\n",
    "- **Learning Rate Scheduling**: Cosine annealing for stable convergence\n",
    "\n",
    "Let's dive into the implementation! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets trl bitsandbytes peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup and Environment Configuration\n",
    "\n",
    "**Colab Compatibility**: This notebook automatically detects and configures the available GPU environment. In Colab, typically only one GPU is available, so we ensure optimal single-GPU training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "Current GPU: 0\n",
      "GPU name: NVIDIA H100 NVL\n",
      "GPU memory: 100.0 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Setup - Colab Compatible\n",
    "import torch\n",
    "\n",
    "# Auto-detect GPU configuration for Colab compatibility\n",
    "# In Colab, there's typically only one GPU available, so we don't need to specify CUDA_VISIBLE_DEVICES\n",
    "# Only set CUDA_VISIBLE_DEVICES if you're running on a multi-GPU system and want to restrict to specific GPUs\n",
    "\n",
    "# For multi-GPU systems (uncomment if needed):\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use first available GPU\n",
    "\n",
    "# Verify GPU setup\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU available. This notebook requires a GPU for efficient training.\")\n",
    "    print(\"In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 06:16:33 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-3B-Instruct\n",
      "Using 4-bit quantization for memory efficiency...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb50cb65444819acb0e532ff221050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Model device: cuda:0\n",
      "üìä Tokenizer vocab size: 151,665\n",
      "üî¢ Model parameters: ~1698.7M\n",
      "üî• GPU memory allocated: 0.90 GB\n",
      "üìà GPU memory cached: 1.17 GB\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration and Loading\n",
    "# This section demonstrates loading a quantized model with LoRA for memory-efficient training\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # Compact model suitable for Colab/consumer GPUs\n",
    "# Alternative models you can try:\n",
    "# model_name = \"microsoft/DialoGPT-small\"  # Even smaller for very limited memory\n",
    "# model_name = \"google/gemma-2b\"           # Google's efficient 2B parameter model\n",
    "\n",
    "max_seq_length = 2048  # Sequence length - reduce if you encounter memory issues\n",
    "\n",
    "# 4-bit Quantization Configuration\n",
    "# This reduces memory usage by ~75% with minimal performance loss\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Use float16 for computations\n",
    "    bnb_4bit_use_double_quant=True,      # Double quantization for additional memory savings\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"Using 4-bit quantization for memory efficiency...\")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically distribute model across available devices\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,  # Use float16 to save memory\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has required tokens\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìç Model device: {next(model.parameters()).device}\")\n",
    "print(f\"üìä Tokenizer vocab size: {len(tokenizer):,}\")\n",
    "print(f\"üî¢ Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"üìà GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that:\n",
    "- Only trains ~0.1% of the model parameters (dramatically reducing memory requirements)\n",
    "- Achieves performance comparable to full fine-tuning\n",
    "- Allows easy switching between different task-specific adaptations\n",
    "\n",
    "**Key LoRA Parameters:**\n",
    "- `r`: Rank determines adaptation capacity (higher = more parameters but better adaptation)\n",
    "- `alpha`: Scaling factor that controls the magnitude of LoRA updates\n",
    "- `target_modules`: Which attention layers to adapt (query and value projections work best)\n",
    "- `dropout`: Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration to model...\n",
      "üìä LoRA Training Parameters Summary:\n",
      "trainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193\n",
      "\n",
      "üéØ Training efficiency:\n",
      "   Total parameters: 1,702,359,040\n",
      "   Trainable parameters: 3,686,400\n",
      "   Percentage trainable: 0.217%\n",
      "   Memory reduction: ~99.8% compared to full fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# LoRA Configuration for Mathematical Reasoning\n",
    "# Optimized for mathematical reasoning tasks that require precise attention patterns\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                              # Rank: Balance between efficiency and adaptation capacity\n",
    "    lora_alpha=32,                     # Alpha: 2x rank is a good starting point\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Target query and value projections for attention\n",
    "    lora_dropout=0.1,                  # Dropout for regularization\n",
    "    bias=\"none\",                       # No bias adaptation to keep it simple\n",
    "    task_type=TaskType.CAUSAL_LM,      # Causal language modeling task\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA configuration to model...\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Display trainable parameters\n",
    "print(\"üìä LoRA Training Parameters Summary:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Calculate the percentage of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"\\nüéØ Training efficiency:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Percentage trainable: {percentage:.3f}%\")\n",
    "print(f\"   Memory reduction: ~{100-percentage:.1f}% compared to full fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation: GSM8K Mathematical Reasoning\n",
    "\n",
    "**About GSM8K**: The Grade School Math 8K dataset contains 8,500+ mathematical word problems that require multi-step reasoning. Each problem:\n",
    "- Tests elementary mathematical concepts (addition, subtraction, multiplication, division)\n",
    "- Requires 2-8 reasoning steps to solve\n",
    "- Has a clear numerical answer that can be objectively evaluated\n",
    "- Includes step-by-step solution explanations\n",
    "\n",
    "**Our Structured Format**: We train the model to generate responses with clear reasoning sections:\n",
    "1. `<start_working_out>` ... `<end_working_out>`: Step-by-step mathematical reasoning\n",
    "2. `<SOLUTION>` ... `</SOLUTION>`: Final numerical answer\n",
    "\n",
    "This structure ensures the model shows its work and provides interpretable solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading GSM8K dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d856d773604341c2b841e2a1b5b11c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded and processed!\n",
      "üìä Training examples: 7,473\n",
      "üéØ Sample question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How m...\n",
      "üéØ Sample answer: 72\n",
      "\n",
      "üìã Complete example format:\n",
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Expected Answer: 72\n",
      "System Prompt: You are a mathematical reasoning assistant.\n",
      "When given a math problem:\n",
      "1. Show your step-by-step work between <start_working_out> and <end_working_out>\n",
      "2. Provide your final numerical answer between <SOLUTION> and </SOLUTION>\n",
      "3. Be precise and show all calculation steps clearly.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Loading and Processing\n",
    "# We structure the dataset to promote clear mathematical reasoning\n",
    "\n",
    "# Define reasoning format tokens for structured output\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "# System prompt that teaches the model our desired reasoning format\n",
    "system_prompt = f\"\"\"You are a mathematical reasoning assistant.\n",
    "When given a math problem:\n",
    "1. Show your step-by-step work between {reasoning_start} and {reasoning_end}\n",
    "2. Provide your final numerical answer between {solution_start} and {solution_end}\n",
    "3. Be precise and show all calculation steps clearly.\"\"\"\n",
    "\n",
    "def extract_hash_answer(text):\n",
    "    \"\"\"Extract numerical answer from GSM8K format (after #### marker)\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    # GSM8K answers are formatted as \"#### 42\" \n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def process_dataset_example(example):\n",
    "    \"\"\"Convert GSM8K example to our training format\"\"\"\n",
    "    question = example[\"question\"]\n",
    "    answer = extract_hash_answer(example[\"answer\"])\n",
    "    \n",
    "    # Create conversation format with system prompt\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"answer\": answer,  # Ground truth numerical answer for evaluation\n",
    "    }\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"üîÑ Loading GSM8K dataset...\")\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "# Process the dataset for our training format\n",
    "dataset = dataset.map(process_dataset_example)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded and processed!\")\n",
    "print(f\"üìä Training examples: {len(dataset):,}\")\n",
    "print(f\"üéØ Sample question: {dataset[0]['prompt'][1]['content'][:100]}...\")\n",
    "print(f\"üéØ Sample answer: {dataset[0]['answer']}\")\n",
    "\n",
    "# Show a complete example\n",
    "print(f\"\\nüìã Complete example format:\")\n",
    "print(f\"Question: {dataset[0]['prompt'][1]['content']}\")\n",
    "print(f\"Expected Answer: {dataset[0]['answer']}\")\n",
    "print(f\"System Prompt: {system_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Reward System: The Heart of Advanced GRPO\n",
    "\n",
    "**Why Multiple Rewards?** Traditional RL uses a single reward signal, but mathematical reasoning requires multiple criteria:\n",
    "\n",
    "1. **Format Compliance**: Does the response follow the required structure?\n",
    "2. **Mathematical Accuracy**: Is the final answer numerically correct?\n",
    "3. **Reasoning Quality**: Are the intermediate steps logical and clear?\n",
    "4. **Robustness**: Can the model handle edge cases and variations?\n",
    "\n",
    "**Our 4-Reward System:**\n",
    "- `match_format_exactly`: High reward (3.0) for perfect format adherence\n",
    "- `match_format_approximately`: Partial rewards (¬±0.5) for near-correct formatting  \n",
    "- `check_answer_correctness`: Graduated rewards (3.0 ‚Üí 1.5 ‚Üí 0.5 ‚Üí -1.0) based on answer quality\n",
    "- `check_numbers_extraction`: Binary reward (1.5 or 0.0) for successful number extraction\n",
    "\n",
    "This multi-objective approach ensures the model learns both **structure** and **content** simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward functions defined\n"
     ]
    }
   ],
   "source": [
    "# Regex patterns for reward functions\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"Reward for exact format matching\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        score = 3.0 if match_format.search(response) is not None else 0.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"Reward for approximate format matching\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        score = 0\n",
    "        \n",
    "        # Count occurrences of format tokens\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer_correctness(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Reward for correct answers\"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Exact match gets full points\n",
    "        if guess == true_answer:\n",
    "            scores.append(3.0)\n",
    "        # Strip whitespace and try again\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            scores.append(1.5)\n",
    "        else:\n",
    "            # Try numerical comparison\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if 0.9 <= ratio <= 1.1:\n",
    "                    scores.append(0.5)\n",
    "                elif 0.8 <= ratio <= 1.2:\n",
    "                    scores.append(0.25)\n",
    "                else:\n",
    "                    scores.append(-1.0)\n",
    "            except:\n",
    "                scores.append(-0.5)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def check_numbers_extraction(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Reward for extracting numbers from solution\"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    print('*' * 20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    \n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"Reward functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration: Optimized for Mathematical Reasoning\n",
    "\n",
    "**GRPO Training Parameters**: These settings are specifically tuned for mathematical reasoning tasks:\n",
    "\n",
    "- **Learning Rate (5e-6)**: Conservative rate prevents overfitting on mathematical patterns\n",
    "- **Batch Size Strategy**: Small per-device batches (2) with high gradient accumulation (8) for memory efficiency\n",
    "- **Sequence Lengths**: Balanced prompt (1024) and completion (1024) lengths for reasoning tasks\n",
    "- **Optimizer**: AdamW with weight decay for stable convergence\n",
    "- **Scheduler**: Cosine annealing for smooth learning rate decay\n",
    "\n",
    "**Memory Optimization for Colab/Consumer GPUs**:\n",
    "- Gradient checkpointing reduces memory by ~50%\n",
    "- Mixed precision training (float16) saves additional memory\n",
    "- Small batch sizes prevent OOM errors on 16GB GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ GRPO Training Configuration Summary:\n",
      "   Learning rate: 5e-06\n",
      "   Per-device batch size: 2\n",
      "   Gradient accumulation: 8\n",
      "   Effective batch size: 16\n",
      "   Max training steps: 10\n",
      "   Max prompt length: 1024\n",
      "   Max completion length: 1024\n",
      "   Optimizer: OptimizerNames.ADAMW_TORCH_FUSED\n",
      "   LR scheduler: SchedulerType.COSINE\n",
      "   Weight decay: 0.1\n",
      "\n",
      "üíæ Memory Optimization:\n",
      "   Gradient checkpointing: Enabled (in model config)\n",
      "   Mixed precision: float16 (from quantization)\n",
      "   Expected GPU memory usage: ~8-12GB\n"
     ]
    }
   ],
   "source": [
    "# GRPO Training Configuration\n",
    "# Optimized for mathematical reasoning with memory-efficient settings\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    # Core learning parameters\n",
    "    learning_rate=5e-6,              # Conservative LR for stable mathematical reasoning\n",
    "    adam_beta1=0.9,                  # Adam optimizer beta1\n",
    "    adam_beta2=0.99,                 # Adam optimizer beta2 \n",
    "    weight_decay=0.1,                # Regularization to prevent overfitting\n",
    "    warmup_ratio=0.1,                # 10% warmup for stable training start\n",
    "    lr_scheduler_type=\"cosine\",      # Cosine annealing for smooth decay\n",
    "    optim=\"adamw_torch_fused\",       # Fused AdamW for better performance\n",
    "    \n",
    "    # Memory and batch size settings (Colab-optimized)\n",
    "    per_device_train_batch_size=2,   # Small batch size for memory efficiency\n",
    "    gradient_accumulation_steps=8,   # Maintain effective batch size of 16\n",
    "    max_prompt_length=1024,          # Reasonable prompt length for math problems\n",
    "    max_completion_length=1024,      # Sufficient space for detailed reasoning\n",
    "    \n",
    "    # Training duration and checkpointing\n",
    "    max_steps=100,                    # Short training for demo (increase for production)\n",
    "    save_steps=100,                   # Save checkpoint every 100 steps\n",
    "    eval_steps=100,                    # Evaluate every 100 steps for detailed monitoring\n",
    "\n",
    "    # Stability and performance\n",
    "    max_grad_norm=0.1,               # Gradient clipping for training stability\n",
    "    dataloader_drop_last=True,       # Consistent batch sizes\n",
    "    \n",
    "    # Logging and output\n",
    "    logging_steps=1,                 # Log every step for detailed monitoring\n",
    "    output_dir=\"./trl_grpo_outputs\", # Output directory for checkpoints\n",
    "    logging_dir=\"./logs\",            # Directory for tensorboard logs\n",
    "    report_to=\"none\",                # Disable external reporting for simplicity\n",
    "    log_level=\"info\",                # Detailed logging\n",
    "    logging_first_step=True,         # Log the first step\n",
    "    logging_nan_inf_filter=True,     # Filter out NaN/inf values in logs\n",
    "    \n",
    "    # Evaluation settings\n",
    "    metric_for_best_model=\"reward\",  # Use reward as the primary metric\n",
    "    greater_is_better=True,          # Higher rewards are better\n",
    "    disable_tqdm=False,              # Keep progress bar enabled\n",
    ")\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"üéØ GRPO Training Configuration Summary:\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Per-device batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Max training steps: {training_args.max_steps}\")\n",
    "print(f\"   Max prompt length: {training_args.max_prompt_length}\")\n",
    "print(f\"   Max completion length: {training_args.max_completion_length}\")\n",
    "print(f\"   Optimizer: {training_args.optim}\")\n",
    "print(f\"   LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"   Weight decay: {training_args.weight_decay}\")\n",
    "\n",
    "# Memory estimation\n",
    "print(f\"\\nüíæ Memory Optimization:\")\n",
    "print(f\"   Gradient checkpointing: Enabled (in model config)\")\n",
    "print(f\"   Mixed precision: float16 (from quantization)\")\n",
    "print(f\"   Expected GPU memory usage: ~8-12GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import TrainerCallback\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import io\n",
    "import sys\n",
    "\n",
    "class HuggingFaceStyleTableCallback(TrainerCallback):\n",
    "    \"\"\"Callback that displays a HuggingFace-style interactive table that updates in-place below progress bar\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file=\"training_logs.json\"):\n",
    "        self.recent_rewards = deque(maxlen=5)\n",
    "        self.best_reward = float('-inf')\n",
    "        self.log_file = log_file\n",
    "        self.all_logs = []\n",
    "        self.metrics_data = []\n",
    "        self.table_displayed = False\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Initialize logging\"\"\"\n",
    "        self.all_logs = []\n",
    "        self.metrics_data = []\n",
    "        self.table_displayed = False\n",
    "        print(f\"Training started - HuggingFace-style interactive table will appear below progress bar\")\n",
    "        print(f\"All logs saved to: {self.log_file}\")\n",
    "        \n",
    "    def _display_hf_style_table(self):\n",
    "        \"\"\"Display HuggingFace-style table that updates in-place\"\"\"\n",
    "        if not self.metrics_data:\n",
    "            return\n",
    "            \n",
    "        # Create DataFrame from metrics data\n",
    "        df = pd.DataFrame(self.metrics_data)\n",
    "        \n",
    "        # Style similar to HuggingFace trainer tables\n",
    "        styled_html = f\"\"\"\n",
    "        <div style=\"margin: 10px 0;\">\n",
    "        <table style=\"border-collapse: collapse; margin: auto; width: 100%; max-width: 1000px;\">\n",
    "        <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #dee2e6;\">\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Step</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Training-Loss</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward-Avg</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward-Std</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward-Best</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Grad-Norm</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">KL-Div</th>\n",
    "        </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            styled_html += f\"\"\"\n",
    "            <tr style=\"{''}\">\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{int(row['Step'])}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['Training-Loss']:.6f}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['Reward']:.6f}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['Reward-Avg']:.6f}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['Reward-Std']:.6f}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['Reward-Best']:.6f}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['Grad-Norm']:.6f}</td>\n",
    "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">{row['KL-Div']:.6f}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        styled_html += \"\"\"\n",
    "        </tbody>\n",
    "        </table>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use clear_output to update in-place like HuggingFace trainers do\n",
    "        if self.table_displayed:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        print(\"TRAINING METRICS:\")\n",
    "        display(HTML(styled_html))\n",
    "        self.table_displayed = True\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Log metrics and update interactive table in-place\"\"\"\n",
    "        if logs is None:\n",
    "            return\n",
    "            \n",
    "        current_step = state.global_step\n",
    "        \n",
    "        # Save complete logs to JSON file with Unix timestamp\n",
    "        log_entry = {\n",
    "            \"step\": current_step,\n",
    "            \"timestamp\": time.time(),\n",
    "            **logs\n",
    "        }\n",
    "        self.all_logs.append(log_entry)\n",
    "        \n",
    "        # Write to JSON file after each step\n",
    "        try:\n",
    "            with open(self.log_file, 'w') as f:\n",
    "                json.dump(self.all_logs, f, indent=2)\n",
    "        except Exception as e:\n",
    "            pass  # Silent fail\n",
    "        \n",
    "        # Extract metrics from logs\n",
    "        reward = logs.get('reward', 0.0)\n",
    "        reward_std = logs.get('reward_std', 0.0)\n",
    "        loss = logs.get('loss', 0.0)\n",
    "        kl_div = logs.get('kl', 0.0)\n",
    "        grad_norm = logs.get('grad_norm', 0.0)\n",
    "        \n",
    "        # Track enhanced metrics\n",
    "        if reward != 0:\n",
    "            self.recent_rewards.append(reward)\n",
    "            if reward > self.best_reward:\n",
    "                self.best_reward = reward\n",
    "                \n",
    "        reward_avg = sum(self.recent_rewards) / len(self.recent_rewards) if self.recent_rewards else 0.0\n",
    "        \n",
    "        # Add new row to metrics data\n",
    "        new_row = {\n",
    "            'Step': current_step,\n",
    "            'Training-Loss': loss,\n",
    "            'Reward': reward,\n",
    "            'Reward-Avg': reward_avg,\n",
    "            'Reward-Std': reward_std,\n",
    "            'Reward-Best': self.best_reward,\n",
    "            'Grad-Norm': grad_norm,\n",
    "            'KL-Div': kl_div,\n",
    "        }\n",
    "        self.metrics_data.append(new_row)\n",
    "        \n",
    "        # Update the table in-place\n",
    "        self._display_hf_style_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Initialize GRPO trainer with HuggingFace-style interactive table callback\n",
    "hf_table_callback = HuggingFaceStyleTableCallback()\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer_correctness,\n",
    "        check_numbers_extraction,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[hf_table_callback],  # Add HuggingFace-style table callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize GRPO trainer without HuggingFace-style table callback\n",
    "# Uncomment the following lines if you want to use the original GRPOTrainer without the interactive table\n",
    "\n",
    "# trainer = GRPOTrainer(\n",
    "#     model=model,\n",
    "#     processing_class=tokenizer,\n",
    "#     reward_funcs=[\n",
    "#         match_format_exactly,\n",
    "#         match_format_approximately,\n",
    "#         check_answer_correctness,\n",
    "#         check_numbers_extraction,\n",
    "#     ],\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset,\n",
    "# )\n",
    "\n",
    "# print(\"Trainer initialized!\")\n",
    "# print(f\"Model device: {model.device}\")\n",
    "# print(f\"Number of training examples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training: Interactive GRPO with Real-Time Metrics\n",
    "\n",
    "**What to Expect During Training:**\n",
    "1. **Initial Steps**: Rewards may be low/negative as the model learns the format\n",
    "2. **Format Learning**: First, the model learns to use the required structure\n",
    "3. **Content Improvement**: Then, mathematical accuracy gradually improves\n",
    "4. **Convergence**: Both format and content rewards should increase together\n",
    "\n",
    "**Monitoring Progress:**\n",
    "- Watch the **\"Reward\"** column for overall performance\n",
    "- **\"Reward-Avg\"** shows the moving average over recent steps\n",
    "- **\"Reward-Best\"** tracks the highest reward achieved\n",
    "- Low **\"KL-Div\"** indicates stable learning (not deviating too much from base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING METRICS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin: 10px 0;\">\n",
       "        <table style=\"border-collapse: collapse; margin: auto; width: 100%; max-width: 1000px;\">\n",
       "        <thead>\n",
       "        <tr style=\"border-bottom: 2px solid #dee2e6;\">\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Step</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Training-Loss</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward-Avg</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward-Std</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Reward-Best</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">Grad-Norm</th>\n",
       "        <th style=\"padding: 12px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;\">KL-Div</th>\n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">1</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">-0.057200</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">2.718750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">2.718750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">2.306226</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">2.718750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.078522</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">2</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.066100</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.218750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.468750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.064589</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.218750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.076023</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.074500</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.062500</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.666667</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.308233</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.218750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.087921</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.002200</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.048910</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.148691</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">-0.009200</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.331250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.322756</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.097224</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">6</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">-0.027800</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.531250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.493750</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.216960</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.104116</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">7</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.009600</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">2.937500</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.237500</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.683079</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.096031</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">8</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">-0.017400</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.640625</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">4.153125</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.819517</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.072870</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">9</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">-0.009300</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.062500</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.765625</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.002159</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.077211</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">10</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.027500</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.365625</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.295985</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.115191</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr style=\"\">\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">10</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">3.365625</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">5.656250</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            <td style=\"padding: 8px; text-align: center; border: 1px solid #dee2e6;\">0.000000</td>\n",
       "            </tr>\n",
       "            \n",
       "        </tbody>\n",
       "        </table>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting GRPO training...\")\n",
    "print(\"This may take a while. Monitor the reward column for improvements.\")\n",
    "print(\"Initial rewards might be low/negative - this is normal.\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Model\n",
    "\n",
    "**Testing Strategy**: We evaluate the model on both simple and complex mathematical problems to assess:\n",
    "1. **Format Adherence**: Does it use the required reasoning structure?\n",
    "2. **Mathematical Accuracy**: Are the calculations correct?\n",
    "3. **Reasoning Quality**: Is the step-by-step logic sound?\n",
    "4. **Robustness**: Can it handle variations in problem types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Testing and Evaluation\n",
    "# Comprehensive testing function with proper generation parameters\n",
    "\n",
    "def test_model(question, max_length=512):\n",
    "    \"\"\"\n",
    "    Test the trained model on a mathematical question\n",
    "    \n",
    "    Args:\n",
    "        question (str): Mathematical question to solve\n",
    "        max_length (int): Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        str: Model's response with reasoning and solution\n",
    "    \"\"\"\n",
    "    # Prepare the conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    # Format using the tokenizer's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    # Tokenize and move to device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate response with optimized parameters\n",
    "    print(f\"ü§î Thinking about: {question}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,                    # Moderate randomness for reasoning\n",
    "            do_sample=True,                     # Enable sampling for diverse reasoning\n",
    "            top_p=0.9,                         # Nucleus sampling for quality\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,            # Reduce repetition in reasoning\n",
    "            length_penalty=1.0,                # Neutral length preference\n",
    "            early_stopping=True,               # Stop at natural endpoints\n",
    "        )\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the newly generated part (after the prompt)\n",
    "    generated_text = response[len(text):].strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with GSM8K-Style Problem\n",
    "\n",
    "Let's test with a more complex word problem that requires multi-step reasoning, similar to the GSM8K dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping'].\n",
      "- `early_stopping`: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Testing Complex Mathematical Reasoning:\n",
      "ü§î Thinking about: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Model Response:\n",
      "_working_out>\n",
      "\n",
      "First, we need to determine how many clips Natalia sold in May. According to the problem, Natalia sold half as many clips in May compared to April. Since she sold 48 clips in April:\n",
      "\n",
      "- Number of clips sold in May = \\(\\frac{48}{2} = 24\\)\n",
      "\n",
      "Next, we add the number of clips sold in both months to find out the total number of clips sold:\n",
      "\n",
      "- Total clips sold = Clips sold in April + Clips sold in May\n",
      "\n",
      "Now let‚Äôs perform the addition:\n",
      "\n",
      "- Total clips sold = 48 + 24 = 72\n",
      "\n",
      "<end_working_out>\n",
      "\n",
      "<SOLUTION>\n",
      "72</SOLUTION>\n",
      "\n",
      "üìä Detailed Analysis:\n",
      "   üìã Format Analysis:\n",
      "      ‚úÖ Contains reasoning section: False\n",
      "      ‚úÖ Contains solution section: True\n",
      "   üéØ Extracted Answer: '72'\n",
      "   ‚úÖ Expected Answer: '72'\n",
      "   ‚úÖ Numerical Accuracy: True\n"
     ]
    }
   ],
   "source": [
    "# Test with a Complex GSM8K-Style Problem\n",
    "# This tests multi-step reasoning capabilities\n",
    "\n",
    "print(\"üß† Testing Complex Mathematical Reasoning:\")\n",
    "\n",
    "# Use the actual GSM8K example from our dataset\n",
    "gsm8k_question = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "expected_answer = \"72\"\n",
    "\n",
    "# Test the model\n",
    "gsm8k_response = test_model(gsm8k_question, max_length=768)\n",
    "\n",
    "print(f\"Question: {gsm8k_question}\")\n",
    "print(f\"\\nModel Response:\\n{gsm8k_response}\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\nüìä Detailed Analysis:\")\n",
    "has_reasoning = reasoning_start in gsm8k_response and reasoning_end in gsm8k_response\n",
    "has_solution = solution_start in gsm8k_response and solution_end in gsm8k_response\n",
    "\n",
    "print(f\"   üìã Format Analysis:\")\n",
    "print(f\"      ‚úÖ Contains reasoning section: {has_reasoning}\")\n",
    "print(f\"      ‚úÖ Contains solution section: {has_solution}\")\n",
    "\n",
    "# Extract reasoning if present\n",
    "if has_reasoning:\n",
    "    try:\n",
    "        reasoning_text = gsm8k_response.split(reasoning_start)[1].split(reasoning_end)[0].strip()\n",
    "        print(f\"   ü§î Reasoning Steps:\")\n",
    "        print(f\"      {reasoning_text[:200]}...\" if len(reasoning_text) > 200 else f\"      {reasoning_text}\")\n",
    "    except:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not extract reasoning cleanly\")\n",
    "\n",
    "# Extract solution if present\n",
    "if has_solution:\n",
    "    try:\n",
    "        solution_text = gsm8k_response.split(solution_start)[1].split(solution_end)[0].strip()\n",
    "        print(f\"   üéØ Extracted Answer: '{solution_text}'\")\n",
    "        print(f\"   ‚úÖ Expected Answer: '{expected_answer}'\")\n",
    "        \n",
    "        # Check numerical accuracy\n",
    "        try:\n",
    "            extracted_number = ''.join(filter(str.isdigit, solution_text))\n",
    "            expected_number = ''.join(filter(str.isdigit, expected_answer))\n",
    "            is_correct = extracted_number == expected_number\n",
    "            print(f\"   {'‚úÖ' if is_correct else '‚ùå'} Numerical Accuracy: {is_correct}\")\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not compare numerical values\")\n",
    "            \n",
    "    except:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not extract solution cleanly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory if needed\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Papers and Research\n",
    "- **GRPO Algorithm**: [Group Relative Policy Optimization](https://arxiv.org/abs/2402.03300) - The original GRPO paper introducing group-based relative policy optimization\n",
    "- **GSM8K Dataset**: [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168) - Cobbe et al., OpenAI\n",
    "- **LoRA**: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Hu et al., Microsoft\n",
    "- **QLoRA**: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - Dettmers et al., 4-bit quantization for efficient training\n",
    "\n",
    "### Libraries and Frameworks\n",
    "- **TRL (Transformers Reinforcement Learning)**: [HuggingFace TRL](https://github.com/huggingface/trl) - Official library for RLHF and advanced training techniques\n",
    "- **Transformers**: [HuggingFace Transformers](https://github.com/huggingface/transformers) - State-of-the-art NLP library\n",
    "- **PEFT**: [Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft) - Efficient adaptation methods\n",
    "- **BitsAndBytes**: [8-bit & 4-bit Quantization](https://github.com/TimDettmers/bitsandbytes) - Memory-efficient training\n",
    "\n",
    "### Models Used\n",
    "- **Qwen2.5-3B-Instruct**: [Qwen Model Series](https://github.com/QwenLM/Qwen2.5) - Alibaba's instruction-tuned language model\n",
    "- **Alternative Models**: Gemma-2B, DialoGPT, GPT-2 (configurable in the notebook)\n",
    "\n",
    "### Datasets\n",
    "- **GSM8K**: [OpenAI GSM8K](https://huggingface.co/datasets/openai/gsm8k) - Grade School Math 8K problems dataset\n",
    "- **Format**: Mathematical word problems requiring multi-step reasoning and numerical answers\n",
    "\n",
    "### Key Concepts\n",
    "- **Reinforcement Learning from Human Feedback (RLHF)**: Training language models using reward signals\n",
    "- **Group Relative Policy Optimization**: Advanced RL technique comparing responses in groups rather than absolute scoring\n",
    "- **Structured Generation**: Teaching models to follow specific output formats with reasoning sections\n",
    "- **Multi-Reward Training**: Using multiple reward functions for comprehensive evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behrooz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
